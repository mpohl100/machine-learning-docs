{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This project was started in November 2024 with the goal to get to know machine learning with a hands-on approach. Now it has grown and yielded some results that I'd like to share with you. I think I ended up with an innovative approach to enable configuring machine learning for two reasons: 1) The architecture of the neural network is configured by a yaml file 2) It is possible to run genetic algorithms on the architecture of the neural network effectively enabling the improving of it while one is sleeping which I think is really cool.</p> <p>In my machine learning project implemented in Rust ml_rust I achieved a code base about machine learning in Rust which currently enables in-memory dense layers with the activation functions ReLU, Sigmoid, Tanh and Softmax. The code design enables an easy extension to an exhaustive list of layer types and activation functions. The next step is to implement being able to pass a memory barrier to limit how much RAM the program can use at max which shall prevent crashes because of configuring too large neural network architectures. Of course the place on disk must be sufficient to store the neural network. That is why this machine learning approach is targeted for students/ scientists. On a laptop one has a lot more disk space than memory. That way you can train much bigger models. After that the plan is to bring dense layers to the GPU so that you can leverage graphics cards. When that is finished I plan on increasing the list of layer types and activation functions because then I have settled on the architecture of the rust traits of layers and activation functions.</p>"},{"location":"#neural-networks","title":"Neural Networks","text":"<p>Check out the explanation what Neural Networks are.</p>"},{"location":"#genetic-algorithms-to-generate-neural-networks","title":"Genetic algorithms to generate neural networks","text":"<p>Genetic algorithms</p>"},{"location":"genetic_algorithms/","title":"Genetic Algorithms","text":""},{"location":"genetic_algorithms/#genetic-algorithms-and-neural-networks","title":"Genetic algorithms and neural networks","text":"<p>Genetic algorithms are an adaptation of the algorithm of life. A generation of phenotypes faces a certain challenge and the winners of the competition get to spread their genes to the next generation of phenotypes. With this optimization algorithm one tries to find the best phenotype for the challenge. In our case the challenge is to find the best prediction percentage of a neural network architecture for a given list of samples and targets. That means that the phenotype is the architecture of a neural network and the challenge is to evaluate all samples with this architecture and to find out what percentage of predictions is correct. The more accurate the predictions of a neural network architecture are the more it shapes the next generation of neural network architectures.</p>"},{"location":"genetic_algorithms/#how-the-neural-network-architecture-can-be-altered","title":"How the neural network architecture can be altered","text":""},{"location":"genetic_algorithms/#crossover","title":"Crossover","text":"<p>The crossover operation can be implemented by taking the left half of the one neural network and the right half of another neural network and to put them together via a \"glue\" dense layer so that the dimensions of all layers match.</p>"},{"location":"genetic_algorithms/#mutate","title":"Mutate","text":""},{"location":"genetic_algorithms/#adding-a-layer","title":"Adding a layer","text":"<p>Mutating a neural network architecture can be done by adding a layer in a random position. The dimensions of the neighbouring layers must be adapted so that the dimensions match, otherwise the neural network architecture is invalid</p>"},{"location":"genetic_algorithms/#changing-the-activation-function-of-an-existing-layer","title":"Changing the activation function of an existing layer","text":"<p>Also, one can change the type of activation function of an existing layer.</p>"},{"location":"genetic_algorithms/#removing-a-layer","title":"Removing a layer","text":"<p>It could also be that the neural network architecture has become too big so one also wants to offer to delete a layer and to make the now new neighbours match in dimensions.</p>"},{"location":"genetic_algorithms/#problems-of-genetic-algorithms","title":"Problems of genetic algorithms","text":"<p>The core problem is that every neural network architecture needs to be trained with all samples for numerous epochs, so the runtime of the algorithm can be pretty bad. Also, randomly selected tries of mutations can be neural network architectures that don't make any sense to try for a human machine learning engineer who can avoid paying the runtime cost. This indicates that the neural network architectures should generally not be too big otherwise the algorithm could take a long time to finish. The purpose of this project is to find out for what kind of machine learning problems genetic algorithms can be of help.</p>"},{"location":"neural_networks/","title":"Neural Networks","text":""},{"location":"neural_networks/#neural-networks","title":"Neural Networks","text":""},{"location":"neural_networks/#why-are-neural-networks-useful","title":"Why are neural networks useful","text":"<p>One has an input vector v of dimension 100.  1) In a classification problem one wants to find out to which of three classes the vector v belongs. In this case one wants to transform the vector v to a vector of targets t of dimension three where the sum of the entries is one. The individual entries denote the likelyhood that vector v belongs to the category of the position of the entry in vector t. 2) One can also imagine that one has 100 entries of data and wants to artificially generate another 50 values that are similar to the data in vector v. In this case the target vector t has 50 entries.</p>"},{"location":"neural_networks/#how-does-one-change-the-dimensions-of-vector-v","title":"How does one change the dimensions of vector v?","text":"<p>For this one can use a so called dense layer. A dense layer consist of a matrix of weights W and a vector of biases b. Also some kind of activation function a is used. 1) In scenario one of above the matrix W of weights has the dimensions 100 x 3. Calculation v' = W * v converts the dimensions of vector v from 100 to 3. After that the biases of dimesnion 3 are applied by calculating the sum v'' = v' + b. This is known as the forward pass through the dense layer. Finally the activation function a is applied to all entries of vector v'': v''' = a(v''). The calculations for the dense layer are finished. 2) In scenario two the same calculations are applied except this time the matrix of weights W has the dimensions 100 x 50 and the biases have the dimension 50. Therefore the dimensions of vector v''' are 50.</p>"},{"location":"neural_networks/#how-a-neural-network-is-formed","title":"How a neural network is formed.","text":"<p>A neural network consists of a concatenation of dense layers in our example which transforms the dimensions and the values of the entries for example like this: 1) dense layer 1: 100 -&gt; 50; dense layer 2: 50 -&gt; 25; dense layer 3: 25 -&gt; 12; dense layer 4: 12 -&gt; 6; dense layer 5: 6 -&gt; 3 2) one can also make the neural network bigger in the beginning: dense layer 1: 100 -&gt; 200; dense layer 2: 200 -&gt; 200; dense layer 3: 200 -&gt; 100; dense layer 4: 100 -&gt; 50; FInding out the optimal architecture of the neural network is part of the job of the engineer.</p>"},{"location":"neural_networks/#how-the-weights-and-biases-are-calculated","title":"How the weights and biases are calculated","text":"<p>For this the technique known as linear regression is used. Basically one can compare the calculated output of a dense layer v''' and compare it to the target output t. One can caluclate the difference by calculating d = v''' - t.  1) The derivative of the activation function is applied to get the vector vb'' 2) One can calculate a matrix of gradients by calculating G = vb'' transposed * v'. 3) Now one can calculate the matrix of weights of the next epoch W next = W previous + learning_rate * G. The learning rate is a constant floating point number which defines the speed of regression.</p>"},{"location":"neural_networks/#how-training-is-organized","title":"How training is organized","text":"<p>Above the calculation and the training of one dense layer is described. The training of a neural network now takes for example 100 epochs and over time the difference between the actual outputs and the target outputs is minimized. The loss of each epoch is a measure of how much the weights and biases of the neural network have changed. Over time the loss should become smaller.</p>"},{"location":"neural_networks/#other-types-of-neural-networks","title":"Other Types of Neural Networks","text":"<p>The basic idea of introducing new types of Neural Networks is to make them turing complete.</p>"},{"location":"neural_networks/#retry-neural-network","title":"Retry Neural Network","text":""},{"location":"neural_networks/#what-it-is","title":"What it is","text":"<p>A retry neural network receives an internal dimension in the layers 1..N with N the total number of layers. If after evaluating the primary network the internal dimension is close to zero the secondary neural network which is the same architecture as the primary neural network minus the internal dimensions is invoked and the evaluation of the secondary is returned as the result. Now it is possible that the secondary network is again a retry network and therefore the touring condition that one can loop is fullfilled. Obviously it is tough to loop forever as one would run out of diskspace to save the neural networks. Looping forever is anyways not a practical application of a neural network as one wants to compute a result in an acceptable amount of time. However the gist of retry neural networks is that they are basically able to dismiss their first result and can invoke a secondary network where its confidence to be right is higher. Of course the secondary network can again be a retry network and therefore you can add an arbitrary number pf levels.</p>"},{"location":"neural_networks/#how-to-train-a-retry-network","title":"How to train a retry network","text":"<p>First one trains the primary neural network. Then all the training samples are tested with this neural network. The correct predictions receive a one as last internal dimension which stands for \"do not continue\". The wrong predictions receive a zero which stands for \"continue\". After that the primary network is reshaped to contain the internal dimensions. Now everything needs to be retrained. The theory is that the 0..N-1 dimensions end up being the same as before and the network learns with the internal dimension whether to continue or not. Then only the wrongly predicted samples are passed as training parameters to the secondary neural network. That means with each level of retry you add the training samples become less in numbers. The theory is that the toughest to classify samples get rejected by all levels and reach the last level of retries. The hope is to improve the accuracy of a neural network architecture in general. With just one level it should be worse than with some retry levels.</p>"},{"location":"neural_networks/#branching-neural-network","title":"Branching neural network","text":""},{"location":"neural_networks/#what-it-is_1","title":"What it is","text":"<p>A retry neural network is capable of looping, now the missing feature is branching. This can be implemented by introducing not one but N internal dimensions to the primary neural network. Then one can extract the internal dimensions from the evaluation of a sample in the primary neural network and run a softmax activation layer with temperature 1.0 on it. This method is guaranteed to pick 1 out of the N dimensions. Now the branching neural networks needs to have N other neural networks as member and the sample can be forwarded to the ith neural network. This ith neural network may again be a branching neural network or a classic neural network or a retry neural network.</p>"},{"location":"neural_networks/#how-to-compose-different-neural-network-types","title":"How to compose different neural network types","text":"<p>The idea is to provide a turing.yaml file which configures the branching and looping logic of the desired architecture. It is also planned to let the nn_generator use genetic algorithms to try out different turing configurations.</p>"}]}